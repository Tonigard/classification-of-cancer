{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1659908,"sourceType":"datasetVersion","datasetId":982666}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diskcache==4.1.0","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2024-06-13T09:35:37.845271Z","iopub.execute_input":"2024-06-13T09:35:37.845629Z","iopub.status.idle":"2024-06-13T09:35:51.260642Z","shell.execute_reply.started":"2024-06-13T09:35:37.845600Z","shell.execute_reply":"2024-06-13T09:35:51.259545Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting diskcache==4.1.0\n  Downloading diskcache-4.1.0-py2.py3-none-any.whl.metadata (19 kB)\nDownloading diskcache-4.1.0-py2.py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache\nSuccessfully installed diskcache-4.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#logconf\n\nimport logging\nimport logging.handlers\nimport random\n\nroot_logger = logging.getLogger()\nroot_logger.setLevel(logging.INFO)\n\n# Some libraries attempt to add their own root logger handlers. This is\n# annoying and so we get rid of them.\nfor handler in list(root_logger.handlers):\n    root_logger.removeHandler(handler)\n\nlogfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\nformatter = logging.Formatter(logfmt_str)\n\nstreamHandler = logging.StreamHandler()\nstreamHandler.setFormatter(formatter)\nstreamHandler.setLevel(logging.DEBUG)\n\nroot_logger.addHandler(streamHandler)\n\n#disk\n\nimport gzip\n\nfrom diskcache import FanoutCache, Disk,core\nfrom diskcache.core import io\nfrom io import BytesIO\nfrom diskcache.core import MODE_BINARY\n\n\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\nlog.setLevel(logging.INFO)\n# log.setLevel(logging.DEBUG)\n\n\nclass GzipDisk(Disk):\n    def store(self, value, read, key=None):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param value: value to convert\n        :param bool read: True when value is file-like object\n        :return: (size, mode, filename, value) tuple for Cache table\n        \"\"\"\n        # pylint: disable=unidiomatic-typecheck\n        if type(value) is bytes:\n            if read:\n                value = value.read()\n                read = False\n\n            str_io = BytesIO()\n            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n\n            for offset in range(0, len(value), 2**30):\n                gz_file.write(value[offset:offset+2**30])\n            gz_file.close()\n\n            value = str_io.getvalue()\n\n        return super(GzipDisk, self).store(value, read)\n\n\n    def fetch(self, mode, filename, value, read):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param int mode: value mode raw, binary, text, or pickle\n        :param str filename: filename of corresponding value\n        :param value: database value\n        :param bool read: when True, return an open file handle\n        :return: corresponding Python value\n        \"\"\"\n        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n\n        if mode == MODE_BINARY:\n            str_io = BytesIO(value)\n            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n            read_csio = BytesIO()\n\n            while True:\n                uncompressed_data = gz_file.read(2**30)\n                if uncompressed_data:\n                    read_csio.write(uncompressed_data)\n                else:\n                    break\n\n            value = read_csio.getvalue()\n\n        return value\n\ndef getCache(scope_str):\n    return FanoutCache('data-unversioned/cache/' + scope_str,\n                       disk=GzipDisk,\n                       shards=64,\n                       timeout=1,\n                       size_limit=17e9,\n                       # disk_min_file_size=2**20,\n                       )\n\n# def disk_cache(base_path, memsize=2):\n#     def disk_cache_decorator(f):\n#         @functools.wraps(f)\n#         def wrapper(*args, **kwargs):\n#             args_str = repr(args) + repr(sorted(kwargs.items()))\n#             file_str = hashlib.md5(args_str.encode('utf8')).hexdigest()\n#\n#             cache_path = os.path.join(base_path, f.__name__, file_str + '.pkl.gz')\n#\n#             if not os.path.exists(os.path.dirname(cache_path)):\n#                 os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n#\n#             if os.path.exists(cache_path):\n#                 return pickle_loadgz(cache_path)\n#             else:\n#                 ret = f(*args, **kwargs)\n#                 pickle_dumpgz(cache_path, ret)\n#                 return ret\n#\n#         return wrapper\n#\n#     return disk_cache_decorator\n#\n#\n# def pickle_dumpgz(file_path, obj):\n#     log.debug(\"Writing {}\".format(file_path))\n#     with open(file_path, 'wb') as file_obj:\n#         with gzip.GzipFile(mode='wb', compresslevel=1, fileobj=file_obj) as gz_file:\n#             pickle.dump(obj, gz_file, pickle.HIGHEST_PROTOCOL)\n#\n#\n# def pickle_loadgz(file_path):\n#     log.debug(\"Reading {}\".format(file_path))\n#     with open(file_path, 'rb') as file_obj:\n#         with gzip.GzipFile(mode='rb', fileobj=file_obj) as gz_file:\n#             return pickle.load(gz_file)\n#\n#\n# def dtpath(dt=None):\n#     if dt is None:\n#         dt = datetime.datetime.now()\n#\n#     return str(dt).rsplit('.', 1)[0].replace(' ', '--').replace(':', '.')\n#\n#\n# def safepath(s):\n#     s = s.replace(' ', '_')\n#     return re.sub('[^A-Za-z0-9_.-]', '', s)\n\n\n\n\nimport logging\nimport logging.handlers\n\nroot_logger = logging.getLogger()\nroot_logger.setLevel(logging.INFO)\n\n# Some libraries attempt to add their own root logger handlers. This is\n# annoying and so we get rid of them.\nfor handler in list(root_logger.handlers):\n    root_logger.removeHandler(handler)\n\nlogfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\nformatter = logging.Formatter(logfmt_str)\n\nstreamHandler = logging.StreamHandler()\nstreamHandler.setFormatter(formatter)\nstreamHandler.setLevel(logging.DEBUG)\n\nroot_logger.addHandler(streamHandler)\n\n\n\nimport math\nimport random\nimport warnings\n\nimport numpy as np\nimport scipy.ndimage\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nimport torch.backends.cudnn as cudnn\n\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\n# log.setLevel(logging.INFO)\nlog.setLevel(logging.DEBUG)\n\ndef cropToShape(image, new_shape, center_list=None, fill=0.0):\n    # log.debug([image.shape, new_shape, center_list])\n    # assert len(image.shape) == 3, repr(image.shape)\n\n    if center_list is None:\n        center_list = [int(image.shape[i] / 2) for i in range(3)]\n\n    crop_list = []\n    for i in range(0, 3):\n        crop_int = center_list[i]\n        if image.shape[i] > new_shape[i] and crop_int is not None:\n\n            # We can't just do crop_int +/- shape/2 since shape might be odd\n            # and ints round down.\n            start_int = crop_int - int(new_shape[i]/2)\n            end_int = start_int + new_shape[i]\n            crop_list.append(slice(max(0, start_int), end_int))\n        else:\n            crop_list.append(slice(0, image.shape[i]))\n\n    # log.debug([image.shape, crop_list])\n    image = image[crop_list]\n\n    crop_list = []\n    for i in range(0, 3):\n        if image.shape[i] < new_shape[i]:\n            crop_int = int((new_shape[i] - image.shape[i]) / 2)\n            crop_list.append(slice(crop_int, crop_int + image.shape[i]))\n        else:\n            crop_list.append(slice(0, image.shape[i]))\n\n    # log.debug([image.shape, crop_list])\n    new_image = np.zeros(new_shape, dtype=image.dtype)\n    new_image[:] = fill\n    new_image[crop_list] = image\n\n    return new_image\n\n\ndef zoomToShape(image, new_shape, square=True):\n    # assert image.shape[-1] in {1, 3, 4}, repr(image.shape)\n\n    if square and image.shape[0] != image.shape[1]:\n        crop_int = min(image.shape[0], image.shape[1])\n        new_shape = [crop_int, crop_int, image.shape[2]]\n        image = cropToShape(image, new_shape)\n\n    zoom_shape = [new_shape[i] / image.shape[i] for i in range(3)]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        image = scipy.ndimage.interpolation.zoom(\n            image, zoom_shape,\n            output=None, order=0, mode='nearest', cval=0.0, prefilter=True)\n\n    return image\n\ndef randomOffset(image_list, offset_rows=0.125, offset_cols=0.125):\n\n    center_list = [int(image_list[0].shape[i] / 2) for i in range(3)]\n    center_list[0] += int(offset_rows * (random.random() - 0.5) * 2)\n    center_list[1] += int(offset_cols * (random.random() - 0.5) * 2)\n    center_list[2] = None\n\n    new_list = []\n    for image in image_list:\n        new_image = cropToShape(image, image.shape, center_list)\n        new_list.append(new_image)\n\n    return new_list\n\n\ndef randomZoom(image_list, scale=None, scale_min=0.8, scale_max=1.3):\n    if scale is None:\n        scale = scale_min + (scale_max - scale_min) * random.random()\n\n    new_list = []\n    for image in image_list:\n        # assert image.shape[-1] in {1, 3, 4}, repr(image.shape)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            # log.info([image.shape])\n            zimage = scipy.ndimage.interpolation.zoom(\n                image, [scale, scale, 1.0],\n                output=None, order=0, mode='nearest', cval=0.0, prefilter=True)\n        image = cropToShape(zimage, image.shape)\n\n        new_list.append(image)\n\n    return new_list\n\n\n_randomFlip_transform_list = [\n    # lambda a: np.rot90(a, axes=(0, 1)),\n    # lambda a: np.flip(a, 0),\n    lambda a: np.flip(a, 1),\n]\n\ndef randomFlip(image_list, transform_bits=None):\n    if transform_bits is None:\n        transform_bits = random.randrange(0, 2 ** len(_randomFlip_transform_list))\n\n    new_list = []\n    for image in image_list:\n        # assert image.shape[-1] in {1, 3, 4}, repr(image.shape)\n\n        for n in range(len(_randomFlip_transform_list)):\n            if transform_bits & 2**n:\n                # prhist(image, 'before')\n                image = _randomFlip_transform_list[n](image)\n                # prhist(image, 'after ')\n\n        new_list.append(image)\n\n    return new_list\n\n\ndef randomSpin(image_list, angle=None, range_tup=None, axes=(0, 1)):\n    if range_tup is None:\n        range_tup = (0, 360)\n\n    if angle is None:\n        angle = range_tup[0] + (range_tup[1] - range_tup[0]) * random.random()\n\n    new_list = []\n    for image in image_list:\n        # assert image.shape[-1] in {1, 3, 4}, repr(image.shape)\n\n        image = scipy.ndimage.interpolation.rotate(\n                image, angle, axes=axes, reshape=False,\n                output=None, order=0, mode='nearest', cval=0.0, prefilter=True)\n\n        new_list.append(image)\n\n    return new_list\n\n\ndef randomNoise(image_list, noise_min=-0.1, noise_max=0.1):\n    noise = np.zeros_like(image_list[0])\n    noise += (noise_max - noise_min) * np.random.random_sample(image_list[0].shape) + noise_min\n    noise *= 5\n    noise = scipy.ndimage.filters.gaussian_filter(noise, 3)\n    # noise += (noise_max - noise_min) * np.random.random_sample(image_hsv.shape) + noise_min\n\n    new_list = []\n    for image_hsv in image_list:\n        image_hsv = image_hsv + noise\n\n        new_list.append(image_hsv)\n\n    return new_list\n\n\ndef randomHsvShift(image_list, h=None, s=None, v=None,\n                   h_min=-0.1, h_max=0.1,\n                   s_min=0.5, s_max=2.0,\n                   v_min=0.5, v_max=2.0):\n    if h is None:\n        h = h_min + (h_max - h_min) * random.random()\n    if s is None:\n        s = s_min + (s_max - s_min) * random.random()\n    if v is None:\n        v = v_min + (v_max - v_min) * random.random()\n\n    new_list = []\n    for image_hsv in image_list:\n        # assert image_hsv.shape[-1] == 3, repr(image_hsv.shape)\n\n        image_hsv[:,:,0::3] += h\n        image_hsv[:,:,1::3] = image_hsv[:,:,1::3] ** s\n        image_hsv[:,:,2::3] = image_hsv[:,:,2::3] ** v\n\n        new_list.append(image_hsv)\n\n    return clampHsv(new_list)\n\n\ndef clampHsv(image_list):\n    new_list = []\n    for image_hsv in image_list:\n        image_hsv = image_hsv.clone()\n\n        # Hue wraps around\n        image_hsv[:,:,0][image_hsv[:,:,0] > 1] -= 1\n        image_hsv[:,:,0][image_hsv[:,:,0] < 0] += 1\n\n        # Everything else clamps between 0 and 1\n        image_hsv[image_hsv > 1] = 1\n        image_hsv[image_hsv < 0] = 0\n\n        new_list.append(image_hsv)\n\n    return new_list\n\n\n# def torch_augment(input):\n#     theta = random.random() * math.pi * 2\n#     s = math.sin(theta)\n#     c = math.cos(theta)\n#     c1 = 1 - c\n#     axis_vector = torch.rand(3, device='cpu', dtype=torch.float64)\n#     axis_vector -= 0.5\n#     axis_vector /= axis_vector.abs().sum()\n#     l, m, n = axis_vector\n#\n#     matrix = torch.tensor([\n#         [l*l*c1 +   c, m*l*c1 - n*s, n*l*c1 + m*s, 0],\n#         [l*m*c1 + n*s, m*m*c1 +   c, n*m*c1 - l*s, 0],\n#         [l*n*c1 - m*s, m*n*c1 + l*s, n*n*c1 +   c, 0],\n#         [0, 0, 0, 1],\n#     ], device=input.device, dtype=torch.float32)\n#\n#     return th_affine3d(input, matrix)\n\n\n\n\n# following from https://github.com/ncullen93/torchsample/blob/master/torchsample/utils.py\n# MIT licensed\n\n# def th_affine3d(input, matrix):\n#     \"\"\"\n#     3D Affine image transform on torch.Tensor\n#     \"\"\"\n#     A = matrix[:3,:3]\n#     b = matrix[:3,3]\n#\n#     # make a meshgrid of normal coordinates\n#     coords = th_iterproduct(input.size(-3), input.size(-2), input.size(-1), dtype=torch.float32)\n#\n#     # shift the coordinates so center is the origin\n#     coords[:,0] = coords[:,0] - (input.size(-3) / 2. - 0.5)\n#     coords[:,1] = coords[:,1] - (input.size(-2) / 2. - 0.5)\n#     coords[:,2] = coords[:,2] - (input.size(-1) / 2. - 0.5)\n#\n#     # apply the coordinate transformation\n#     new_coords = coords.mm(A.t().contiguous()) + b.expand_as(coords)\n#\n#     # shift the coordinates back so origin is origin\n#     new_coords[:,0] = new_coords[:,0] + (input.size(-3) / 2. - 0.5)\n#     new_coords[:,1] = new_coords[:,1] + (input.size(-2) / 2. - 0.5)\n#     new_coords[:,2] = new_coords[:,2] + (input.size(-1) / 2. - 0.5)\n#\n#     # map new coordinates using bilinear interpolation\n#     input_transformed = th_trilinear_interp3d(input, new_coords)\n#\n#     return input_transformed\n#\n#\n# def th_trilinear_interp3d(input, coords):\n#     \"\"\"\n#     trilinear interpolation of 3D torch.Tensor image\n#     \"\"\"\n#     # take clamp then floor/ceil of x coords\n#     x = torch.clamp(coords[:,0], 0, input.size(-3)-2)\n#     x0 = x.floor()\n#     x1 = x0 + 1\n#     # take clamp then floor/ceil of y coords\n#     y = torch.clamp(coords[:,1], 0, input.size(-2)-2)\n#     y0 = y.floor()\n#     y1 = y0 + 1\n#     # take clamp then floor/ceil of z coords\n#     z = torch.clamp(coords[:,2], 0, input.size(-1)-2)\n#     z0 = z.floor()\n#     z1 = z0 + 1\n#\n#     stride = torch.tensor(input.stride()[-3:], dtype=torch.int64, device=input.device)\n#     x0_ix = x0.mul(stride[0]).long()\n#     x1_ix = x1.mul(stride[0]).long()\n#     y0_ix = y0.mul(stride[1]).long()\n#     y1_ix = y1.mul(stride[1]).long()\n#     z0_ix = z0.mul(stride[2]).long()\n#     z1_ix = z1.mul(stride[2]).long()\n#\n#     # input_flat = th_flatten(input)\n#     input_flat = x.contiguous().view(x[0], x[1], -1)\n#\n#     vals_000 = input_flat[:, :, x0_ix+y0_ix+z0_ix]\n#     vals_001 = input_flat[:, :, x0_ix+y0_ix+z1_ix]\n#     vals_010 = input_flat[:, :, x0_ix+y1_ix+z0_ix]\n#     vals_011 = input_flat[:, :, x0_ix+y1_ix+z1_ix]\n#     vals_100 = input_flat[:, :, x1_ix+y0_ix+z0_ix]\n#     vals_101 = input_flat[:, :, x1_ix+y0_ix+z1_ix]\n#     vals_110 = input_flat[:, :, x1_ix+y1_ix+z0_ix]\n#     vals_111 = input_flat[:, :, x1_ix+y1_ix+z1_ix]\n#\n#     xd = x - x0\n#     yd = y - y0\n#     zd = z - z0\n#     xm1 = 1 - xd\n#     ym1 = 1 - yd\n#     zm1 = 1 - zd\n#\n#     x_mapped = (\n#             vals_000.mul(xm1).mul(ym1).mul(zm1) +\n#             vals_001.mul(xm1).mul(ym1).mul(zd) +\n#             vals_010.mul(xm1).mul(yd).mul(zm1) +\n#             vals_011.mul(xm1).mul(yd).mul(zd) +\n#             vals_100.mul(xd).mul(ym1).mul(zm1) +\n#             vals_101.mul(xd).mul(ym1).mul(zd) +\n#             vals_110.mul(xd).mul(yd).mul(zm1) +\n#             vals_111.mul(xd).mul(yd).mul(zd)\n#     )\n#\n#     return x_mapped.view_as(input)\n#\n# def th_iterproduct(*args, dtype=None):\n#     return torch.from_numpy(np.indices(args).reshape((len(args),-1)).T)\n#\n# def th_flatten(x):\n#     \"\"\"Flatten tensor\"\"\"\n#     return x.contiguous().view(x[0], x[1], -1)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:51.263226Z","iopub.execute_input":"2024-06-13T09:35:51.263571Z","iopub.status.idle":"2024-06-13T09:35:54.401180Z","shell.execute_reply.started":"2024-06-13T09:35:51.263544Z","shell.execute_reply":"2024-06-13T09:35:54.400198Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#vis.py\n\n# import matplotlib\n# matplotlib.use('nbagg')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclim=(-1000.0, 300)\n\ndef findPositiveSamples(start_ndx=0, limit=10):\n    ds = LunaDataset(sortby_str='label_and_size')\n\n    positiveSample_list = []\n    for sample_tup in ds.candidateInfo_list:\n        if sample_tup.isNodule_bool:\n            print(len(positiveSample_list), sample_tup)\n            positiveSample_list.append(sample_tup)\n\n        if len(positiveSample_list) >= limit:\n            break\n\n    return positiveSample_list\n\ndef showCandidate(series_uid, batch_ndx=None, **kwargs):\n    ds = LunaDataset(series_uid=series_uid, **kwargs)\n    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n\n    if batch_ndx is None:\n        if pos_list:\n            batch_ndx = pos_list[0]\n        else:\n            print(\"Warning: no positive samples found; using first negative sample.\")\n            batch_ndx = 0\n\n    ct = Ct(series_uid)\n    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n    ct_a = ct_t[0].numpy()\n\n    fig = plt.figure(figsize=(30, 50))\n\n    group_list = [\n        [9, 11, 13],\n        [15, 16, 17],\n        [19, 21, 23],\n    ]\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct.hu_a[int(center_irc.index)], clim=clim, cmap='gray')\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct.hu_a[:,int(center_irc.row)], clim=clim, cmap='gray')\n    plt.gca().invert_yaxis()\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct.hu_a[:,:,int(center_irc.col)], clim=clim, cmap='gray')\n    plt.gca().invert_yaxis()\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n    plt.gca().invert_yaxis()\n\n    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n        label.set_fontsize(20)\n    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n    plt.gca().invert_yaxis()\n\n    for row, index_list in enumerate(group_list):\n        for col, index in enumerate(index_list):\n            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n            subplot.set_title('slice {}'.format(index), fontsize=30)\n            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n                label.set_fontsize(20)\n            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n\n\n    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:54.402493Z","iopub.execute_input":"2024-06-13T09:35:54.402882Z","iopub.status.idle":"2024-06-13T09:35:54.426310Z","shell.execute_reply.started":"2024-06-13T09:35:54.402857Z","shell.execute_reply":"2024-06-13T09:35:54.425332Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#util \n\nimport collections\nimport copy\nimport datetime\nimport gc\nimport time\n\n# import torch\nimport numpy as np\n\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\n# log.setLevel(logging.INFO)\nlog.setLevel(logging.DEBUG)\n\nIrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\nXyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n\ndef irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n    cri_a = np.array(coord_irc)[::-1]\n    origin_a = np.array(origin_xyz)\n    vxSize_a = np.array(vxSize_xyz)\n    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n    return XyzTuple(*coords_xyz)\n\ndef xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n    origin_a = np.array(origin_xyz)\n    vxSize_a = np.array(vxSize_xyz)\n    coord_a = np.array(coord_xyz)\n    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n    cri_a = np.round(cri_a)\n    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n\n\ndef importstr(module_str, from_=None):\n    \"\"\"\n    >>> importstr('os')\n    <module 'os' from '.../os.pyc'>\n    >>> importstr('math', 'fabs')\n    <built-in function fabs>\n    \"\"\"\n    if from_ is None and ':' in module_str:\n        module_str, from_ = module_str.rsplit(':')\n\n    module = __import__(module_str)\n    for sub_str in module_str.split('.')[1:]:\n        module = getattr(module, sub_str)\n\n    if from_:\n        try:\n            return getattr(module, from_)\n        except:\n            raise ImportError('{}.{}'.format(module_str, from_))\n    return module\n\n\n# class dotdict(dict):\n#     '''dict where key can be access as attribute d.key -> d[key]'''\n#     @classmethod\n#     def deep(cls, dic_obj):\n#         '''Initialize from dict with deep conversion'''\n#         return cls(dic_obj).deepConvert()\n#\n#     def __getattr__(self, attr):\n#         if attr in self:\n#             return self[attr]\n#         log.error(sorted(self.keys()))\n#         raise AttributeError(attr)\n#         #return self.get(attr, None)\n#     __setattr__= dict.__setitem__\n#     __delattr__= dict.__delitem__\n#\n#\n#     def __copy__(self):\n#         return dotdict(self)\n#\n#     def __deepcopy__(self, memo):\n#         new_dict = dotdict()\n#         for k, v in self.items():\n#             new_dict[k] = copy.deepcopy(v, memo)\n#         return new_dict\n#\n#     # pylint: disable=multiple-statements\n#     def __getstate__(self): return self.__dict__\n#     def __setstate__(self, d): self.__dict__.update(d)\n#\n#     def deepConvert(self):\n#         '''Convert all dicts at all tree levels into dotdict'''\n#         for k, v in self.items():\n#             if type(v) is dict: # pylint: disable=unidiomatic-typecheck\n#                 self[k] = dotdict(v)\n#                 self[k].deepConvert()\n#             try: # try enumerable types\n#                 for m, x in enumerate(v):\n#                     if type(x) is dict: # pylint: disable=unidiomatic-typecheck\n#                         x = dotdict(x)\n#                         x.deepConvert()\n#                         v[m] = x#\n\n#             except TypeError:\n#                 pass\n#         return self\n#\n#     def copy(self):\n#         # override dict.copy()\n#         return dotdict(self)\n\n\ndef prhist(ary, prefix_str=None, **kwargs):\n    if prefix_str is None:\n        prefix_str = ''\n    else:\n        prefix_str += ' '\n\n    count_ary, bins_ary = np.histogram(ary, **kwargs)\n    for i in range(count_ary.shape[0]):\n        print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[i]), \"{:-10}\".format(count_ary[i]))\n    print(\"{}{:-8.2f}\".format(prefix_str, bins_ary[-1]))\n\n# def dumpCuda():\n#     # small_count = 0\n#     total_bytes = 0\n#     size2count_dict = collections.defaultdict(int)\n#     size2bytes_dict = {}\n#     for obj in gc.get_objects():\n#         if isinstance(obj, torch.cuda._CudaBase):\n#             nbytes = 4\n#             for n in obj.size():\n#                 nbytes *= n\n#\n#             size2count_dict[tuple([obj.get_device()] + list(obj.size()))] += 1\n#             size2bytes_dict[tuple([obj.get_device()] + list(obj.size()))] = nbytes\n#\n#             total_bytes += nbytes\n#\n#     # print(small_count, \"tensors equal to or less than than 16 bytes\")\n#     for size, count in sorted(size2count_dict.items(), key=lambda sc: (size2bytes_dict[sc[0]] * sc[1], sc[1], sc[0])):\n#         print('{:4}x'.format(count), '{:10,}'.format(size2bytes_dict[size]), size)\n#     print('{:10,}'.format(total_bytes), \"total bytes\")\n\n\ndef enumerateWithEstimate(\n        iter,\n        desc_str,\n        start_ndx=0,\n        print_ndx=4,\n        backoff=None,\n        iter_len=None,\n):\n    \"\"\"\n    In terms of behavior, `enumerateWithEstimate` is almost identical\n    to the standard `enumerate` (the differences are things like how\n    our function returns a generator, while `enumerate` returns a\n    specialized `<enumerate object at 0x...>`).\n\n    However, the side effects (logging, specifically) are what make the\n    function interesting.\n\n    :param iter: `iter` is the iterable that will be passed into\n        `enumerate`. Required.\n\n    :param desc_str: This is a human-readable string that describes\n        what the loop is doing. The value is arbitrary, but should be\n        kept reasonably short. Things like `\"epoch 4 training\"` or\n        `\"deleting temp files\"` or similar would all make sense.\n\n    :param start_ndx: This parameter defines how many iterations of the\n        loop should be skipped before timing actually starts. Skipping\n        a few iterations can be useful if there are startup costs like\n        caching that are only paid early on, resulting in a skewed\n        average when those early iterations dominate the average time\n        per iteration.\n\n        NOTE: Using `start_ndx` to skip some iterations makes the time\n        spent performing those iterations not be included in the\n        displayed duration. Please account for this if you use the\n        displayed duration for anything formal.\n\n        This parameter defaults to `0`.\n\n    :param print_ndx: determines which loop interation that the timing\n        logging will start on. The intent is that we don't start\n        logging until we've given the loop a few iterations to let the\n        average time-per-iteration a chance to stablize a bit. We\n        require that `print_ndx` not be less than `start_ndx` times\n        `backoff`, since `start_ndx` greater than `0` implies that the\n        early N iterations are unstable from a timing perspective.\n\n        `print_ndx` defaults to `4`.\n\n    :param backoff: This is used to how many iterations to skip before\n        logging again. Frequent logging is less interesting later on,\n        so by default we double the gap between logging messages each\n        time after the first.\n\n        `backoff` defaults to `2` unless iter_len is > 1000, in which\n        case it defaults to `4`.\n\n    :param iter_len: Since we need to know the number of items to\n        estimate when the loop will finish, that can be provided by\n        passing in a value for `iter_len`. If a value isn't provided,\n        then it will be set by using the value of `len(iter)`.\n\n    :return:\n    \"\"\"\n    if iter_len is None:\n        iter_len = len(iter)\n\n    if backoff is None:\n        backoff = 2\n        while backoff ** 7 < iter_len:\n            backoff *= 2\n\n    assert backoff >= 2\n    while print_ndx < start_ndx * backoff:\n        print_ndx *= backoff\n\n    log.warning(\"{} ----/{}, starting\".format(\n        desc_str,\n        iter_len,\n    ))\n    start_ts = time.time()\n    for (current_ndx, item) in enumerate(iter):\n        yield (current_ndx, item)\n        if current_ndx == print_ndx:\n            # ... <1>\n            duration_sec = ((time.time() - start_ts)\n                            / (current_ndx - start_ndx + 1)\n                            * (iter_len-start_ndx)\n                            )\n\n            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n            done_td = datetime.timedelta(seconds=duration_sec)\n\n            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n                desc_str,\n                current_ndx,\n                iter_len,\n                str(done_dt).rsplit('.', 1)[0],\n                str(done_td).rsplit('.', 1)[0],\n            ))\n\n            print_ndx *= backoff\n\n        if current_ndx + 1 == start_ndx:\n            start_ts = time.time()\n\n    log.warning(\"{} ----/{}, done at {}\".format(\n        desc_str,\n        iter_len,\n        str(datetime.datetime.now()).rsplit('.', 1)[0],\n    ))\n\n#\n# try:\n#     import matplotlib\n#     matplotlib.use('agg', warn=False)\n#\n#     import matplotlib.pyplot as plt\n#     # matplotlib color maps\n#     cdict = {'red':   ((0.0,  1.0, 1.0),\n#                        # (0.5,  1.0, 1.0),\n#                        (1.0,  1.0, 1.0)),\n#\n#              'green': ((0.0,  0.0, 0.0),\n#                        (0.5,  0.0, 0.0),\n#                        (1.0,  0.5, 0.5)),\n#\n#              'blue':  ((0.0,  0.0, 0.0),\n#                        # (0.5,  0.5, 0.5),\n#                        # (0.75, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'alpha':  ((0.0, 0.0, 0.0),\n#                        (0.75, 0.5, 0.5),\n#                        (1.0,  0.5, 0.5))}\n#\n#     plt.register_cmap(name='mask', data=cdict)\n#\n#     cdict = {'red':   ((0.0,  0.0, 0.0),\n#                        (0.25,  1.0, 1.0),\n#                        (1.0,  1.0, 1.0)),\n#\n#              'green': ((0.0,  1.0, 1.0),\n#                        (0.25,  1.0, 1.0),\n#                        (0.5, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'blue':  ((0.0,  0.0, 0.0),\n#                        # (0.5,  0.5, 0.5),\n#                        # (0.75, 0.0, 0.0),\n#                        (1.0,  0.0, 0.0)),\n#\n#              'alpha':  ((0.0, 0.15, 0.15),\n#                        (0.5,  0.3, 0.3),\n#                        (0.8,  0.0, 0.0),\n#                        (1.0,  0.0, 0.0))}\n#\n#     plt.register_cmap(name='maskinvert', data=cdict)\n# except ImportError:\n#     pass","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:54.429457Z","iopub.execute_input":"2024-06-13T09:35:54.429774Z","iopub.status.idle":"2024-06-13T09:35:54.456248Z","shell.execute_reply.started":"2024-06-13T09:35:54.429750Z","shell.execute_reply":"2024-06-13T09:35:54.455433Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import copy\nimport csv\nimport functools\nimport glob\nimport math\nimport os\nimport random\n\nfrom collections import namedtuple\n\nimport SimpleITK as sitk\nimport numpy as np\n\nimport torch\nimport torch.cuda\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n\nraw_cache = getCache('part2ch12_raw')\n\nCandidateInfoTuple = namedtuple('CandidateInfoTuple', 'isNodule_bool, diameter_mm, series_uid, center_xyz')\n\n@functools.lru_cache(1)\ndef getCandidateInfoList(requireOnDisk_bool=True):\n    # We construct a set with all series_uids that are present on disk.\n    # This will let us use the data, even if we haven't downloaded all of\n    # the subsets yet.\n    mhd_list = glob.glob('/kaggle/input/luna16/subset[0-0]/subset*/*.mhd')\n    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n\n    diameter_dict = {}\n    with open('/kaggle/input/luna16/annotations.csv', \"r\") as f:\n        for row in list(csv.reader(f))[1:]:\n            series_uid = row[0]\n            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n            annotationDiameter_mm = float(row[4])\n\n            diameter_dict.setdefault(series_uid, []).append(\n                (annotationCenter_xyz, annotationDiameter_mm),\n            )\n\n    candidateInfo_list = []\n    with open('/kaggle/input/luna16/candidates.csv', \"r\") as f:\n        for row in list(csv.reader(f))[1:]:\n            series_uid = row[0]\n\n            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n                continue\n\n            isNodule_bool = bool(int(row[4]))\n            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n\n            candidateDiameter_mm = 0.0\n            for annotation_tup in diameter_dict.get(series_uid, []):\n                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n                for i in range(3):\n                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n                    if delta_mm > annotationDiameter_mm / 4:\n                        break\n                else:\n                    candidateDiameter_mm = annotationDiameter_mm\n                    break\n\n            candidateInfo_list.append(CandidateInfoTuple(\n                isNodule_bool,\n                candidateDiameter_mm,\n                series_uid,\n                candidateCenter_xyz,\n            ))\n\n    candidateInfo_list.sort(reverse=True)\n    return candidateInfo_list\n\nclass Ct:\n    def __init__(self, series_uid):\n        mhd_path = glob.glob(\n            '/kaggle/input/luna16/subset[0-0]/subset*/{}.mhd'.format(series_uid)\n        )[0]\n\n        ct_mhd = sitk.ReadImage(mhd_path)\n        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n\n        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n        # The upper bound nukes any weird hotspots and clamps bone down\n        ct_a.clip(-1000, 1000, ct_a)\n\n        self.series_uid = series_uid\n        self.hu_a = ct_a\n\n        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n\n    def getRawCandidate(self, center_xyz, width_irc):\n        center_irc = xyz2irc(\n            center_xyz,\n            self.origin_xyz,\n            self.vxSize_xyz,\n            self.direction_a,\n        )\n\n        slice_list = []\n        for axis, center_val in enumerate(center_irc):\n            start_ndx = int(round(center_val - width_irc[axis]/2))\n            end_ndx = int(start_ndx + width_irc[axis])\n\n            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n\n            if start_ndx < 0:\n                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n                start_ndx = 0\n                end_ndx = int(width_irc[axis])\n\n            if end_ndx > self.hu_a.shape[axis]:\n                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n                end_ndx = self.hu_a.shape[axis]\n                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n\n            slice_list.append(slice(start_ndx, end_ndx))\n\n        ct_chunk = self.hu_a[tuple(slice_list)]\n\n        return ct_chunk, center_irc\n\n\n@functools.lru_cache(1, typed=True)\ndef getCt(series_uid):\n    return Ct(series_uid)\n\n@raw_cache.memoize(typed=True)\ndef getCtRawCandidate(series_uid, center_xyz, width_irc):\n    ct = getCt(series_uid)\n    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n    return ct_chunk, center_irc\n\ndef getCtAugmentedCandidate(\n        augmentation_dict,\n        series_uid, center_xyz, width_irc,\n        use_cache=True):\n    if use_cache:\n        ct_chunk, center_irc = \\\n            getCtRawCandidate(series_uid, center_xyz, width_irc)\n    else:\n        ct = getCt(series_uid)\n        ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n\n    ct_t = torch.tensor(ct_chunk).unsqueeze(0).unsqueeze(0).to(torch.float32)\n\n    transform_t = torch.eye(4)\n    # ... <1>\n\n    for i in range(3):\n        if 'flip' in augmentation_dict:\n            if random.random() > 0.5:\n                transform_t[i,i] *= -1\n\n        if 'offset' in augmentation_dict:\n            offset_float = augmentation_dict['offset']\n            random_float = (random.random() * 2 - 1)\n            transform_t[i,3] = offset_float * random_float\n\n        if 'scale' in augmentation_dict:\n            scale_float = augmentation_dict['scale']\n            random_float = (random.random() * 2 - 1)\n            transform_t[i,i] *= 1.0 + scale_float * random_float\n\n\n    if 'rotate' in augmentation_dict:\n        angle_rad = random.random() * math.pi * 2\n        s = math.sin(angle_rad)\n        c = math.cos(angle_rad)\n\n        rotation_t = torch.tensor([\n            [c, -s, 0, 0],\n            [s, c, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n        ])\n\n        transform_t @= rotation_t\n\n    affine_t = F.affine_grid(\n            transform_t[:3].unsqueeze(0).to(torch.float32),\n            ct_t.size(),\n            align_corners=False,\n        )\n\n    augmented_chunk = F.grid_sample(\n            ct_t,\n            affine_t,\n            padding_mode='border',\n            align_corners=False,\n        ).to('cpu')\n\n    if 'noise' in augmentation_dict:\n        noise_t = torch.randn_like(augmented_chunk)\n        noise_t *= augmentation_dict['noise']\n\n        augmented_chunk += noise_t\n\n    return augmented_chunk[0], center_irc\n\n\nclass LunaDataset(Dataset):\n    def __init__(self,\n                 val_stride=0,\n                 isValSet_bool=None,\n                 series_uid=None,\n                 sortby_str='random',\n                 ratio_int=0,\n                 augmentation_dict=None,\n                 candidateInfo_list=None,\n            ):\n        self.ratio_int = ratio_int\n        self.augmentation_dict = augmentation_dict\n\n        if candidateInfo_list:\n            self.candidateInfo_list = copy.copy(candidateInfo_list)\n            self.use_cache = False\n        else:\n            self.candidateInfo_list = copy.copy(getCandidateInfoList())\n            self.use_cache = True\n\n        if series_uid:\n            self.candidateInfo_list = [\n                x for x in self.candidateInfo_list if x.series_uid == series_uid\n            ]\n\n        if isValSet_bool:\n            assert val_stride > 0, val_stride\n            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n            assert self.candidateInfo_list\n        elif val_stride > 0:\n            del self.candidateInfo_list[::val_stride]\n            assert self.candidateInfo_list\n\n        if sortby_str == 'random':\n            random.shuffle(self.candidateInfo_list)\n        elif sortby_str == 'series_uid':\n            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n        elif sortby_str == 'label_and_size':\n            pass\n        else:\n            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n\n        self.negative_list = [\n            nt for nt in self.candidateInfo_list if not nt.isNodule_bool\n        ]\n        self.pos_list = [\n            nt for nt in self.candidateInfo_list if nt.isNodule_bool\n        ]\n\n        log.info(\"{!r}: {} {} samples, {} neg, {} pos, {} ratio\".format(\n            self,\n            len(self.candidateInfo_list),\n            \"validation\" if isValSet_bool else \"training\",\n            len(self.negative_list),\n            len(self.pos_list),\n            '{}:1'.format(self.ratio_int) if self.ratio_int else 'unbalanced'\n        ))\n\n    def shuffleSamples(self):\n        if self.ratio_int:\n            random.shuffle(self.negative_list)\n            random.shuffle(self.pos_list)\n\n    def __len__(self):\n        if self.ratio_int:\n            return 50000\n        else:\n            return len(self.candidateInfo_list)\n\n    def __getitem__(self, ndx):\n        if self.ratio_int:\n            pos_ndx = ndx // (self.ratio_int + 1)\n\n            if ndx % (self.ratio_int + 1):\n                neg_ndx = ndx - 1 - pos_ndx\n                neg_ndx %= len(self.negative_list)\n                candidateInfo_tup = self.negative_list[neg_ndx]\n            else:\n                pos_ndx %= len(self.pos_list)\n                candidateInfo_tup = self.pos_list[pos_ndx]\n        else:\n            candidateInfo_tup = self.candidateInfo_list[ndx]\n\n        width_irc = (32, 48, 48)\n\n        if self.augmentation_dict:\n            candidate_t, center_irc = getCtAugmentedCandidate(\n                self.augmentation_dict,\n                candidateInfo_tup.series_uid,\n                candidateInfo_tup.center_xyz,\n                width_irc,\n                self.use_cache,\n            )\n        elif self.use_cache:\n            candidate_a, center_irc = getCtRawCandidate(\n                candidateInfo_tup.series_uid,\n                candidateInfo_tup.center_xyz,\n                width_irc,\n            )\n            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n            candidate_t = candidate_t.unsqueeze(0)\n        else:\n            ct = getCt(candidateInfo_tup.series_uid)\n            candidate_a, center_irc = ct.getRawCandidate(\n                candidateInfo_tup.center_xyz,\n                width_irc,\n            )\n            candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n            candidate_t = candidate_t.unsqueeze(0)\n\n        pos_t = torch.tensor([\n                not candidateInfo_tup.isNodule_bool,\n                candidateInfo_tup.isNodule_bool\n            ],\n            dtype=torch.long,\n        )\n\n        return candidate_t, pos_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:54.457680Z","iopub.execute_input":"2024-06-13T09:35:54.457982Z","iopub.status.idle":"2024-06-13T09:35:56.976210Z","shell.execute_reply.started":"2024-06-13T09:35:54.457958Z","shell.execute_reply":"2024-06-13T09:35:56.975415Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def run(app, *argv):\n    argv = list(argv)\n    argv.insert(0, '--num-workers=4')  # <1>\n    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n    \n    app_cls = app  # <2>\n    app_cls(argv).main()\n    \n    log.info(\"Finished: {}.{!r}).main()\".format(app, argv))\n    \nimport os\n\n# Получение количества ядер процессора\nnum_cores = os.cpu_count()\nprint(f'Number of CPU cores: {num_cores}')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:56.977348Z","iopub.execute_input":"2024-06-13T09:35:56.977689Z","iopub.status.idle":"2024-06-13T09:35:56.984233Z","shell.execute_reply.started":"2024-06-13T09:35:56.977660Z","shell.execute_reply":"2024-06-13T09:35:56.983226Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of CPU cores: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\n\nfrom torch import nn as nn\n\n\n\nclass LunaModel(nn.Module):\n    def __init__(self, in_channels=1, conv_channels=8):\n        super().__init__()\n\n        self.tail_batchnorm = nn.BatchNorm3d(1)\n\n        self.block1 = LunaBlock(in_channels, conv_channels)\n        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n\n        self.head_linear = nn.Linear(1152, 2)\n        self.head_softmax = nn.Softmax(dim=1)\n\n        self._init_weights()\n\n    # see also https://github.com/pytorch/pytorch/issues/18182\n    def _init_weights(self):\n        for m in self.modules():\n            if type(m) in {\n                nn.Linear,\n                nn.Conv3d,\n                nn.Conv2d,\n                nn.ConvTranspose2d,\n                nn.ConvTranspose3d,\n            }:\n                nn.init.kaiming_normal_(\n                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n                )\n                if m.bias is not None:\n                    fan_in, fan_out = \\\n                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n                    bound = 1 / math.sqrt(fan_out)\n                    nn.init.normal_(m.bias, -bound, bound)\n\n\n\n    def forward(self, input_batch):\n        bn_output = self.tail_batchnorm(input_batch)\n\n        block_out = self.block1(bn_output)\n        block_out = self.block2(block_out)\n        block_out = self.block3(block_out)\n        block_out = self.block4(block_out)\n\n        conv_flat = block_out.view(\n            block_out.size(0),\n            -1,\n        )\n        linear_output = self.head_linear(conv_flat)\n\n        return linear_output, self.head_softmax(linear_output)\n\n\nclass LunaBlock(nn.Module):\n    def __init__(self, in_channels, conv_channels):\n        super().__init__()\n\n        self.conv1 = nn.Conv3d(\n            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n        )\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(\n            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n        )\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.maxpool = nn.MaxPool3d(2, 2)\n\n    def forward(self, input_batch):\n        block_out = self.conv1(input_batch)\n        block_out = self.relu1(block_out)\n        block_out = self.conv2(block_out)\n        block_out = self.relu2(block_out)\n\n        return self.maxpool(block_out)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:56.985446Z","iopub.execute_input":"2024-06-13T09:35:56.985754Z","iopub.status.idle":"2024-06-13T09:35:57.001373Z","shell.execute_reply.started":"2024-06-13T09:35:56.985718Z","shell.execute_reply":"2024-06-13T09:35:57.000537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport datetime\nimport os\nimport sys\n\nimport numpy as np\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nlog = logging.getLogger(__name__)\n# log.setLevel(logging.WARN)\nlog.setLevel(logging.INFO)\n# log.setLevel(logging.DEBUG)\n\n# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\nMETRICS_LABEL_NDX=0\nMETRICS_PRED_NDX=1\nMETRICS_LOSS_NDX=2\nMETRICS_SIZE = 3\n\nclass LunaTrainingApp:\n    def __init__(self, sys_argv=None):\n        if sys_argv is None:\n            sys_argv = sys.argv[1:]\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--batch-size',\n            help='Batch size to use for training',\n            default=32,\n            type=int,\n        )\n        parser.add_argument('--num-workers',\n            help='Number of worker processes for background data loading',\n            default=8,\n            type=int,\n        )\n        parser.add_argument('--epochs',\n            help='Number of epochs to train for',\n            default=1,\n            type=int,\n        )\n        parser.add_argument('--balanced',\n            help=\"Balance the training data to half positive, half negative.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augmented',\n            help=\"Augment the training data.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augment-flip',\n            help=\"Augment the training data by randomly flipping the data left-right, up-down, and front-back.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augment-offset',\n            help=\"Augment the training data by randomly offsetting the data slightly along the X and Y axes.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augment-scale',\n            help=\"Augment the training data by randomly increasing or decreasing the size of the candidate.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augment-rotate',\n            help=\"Augment the training data by randomly rotating the data around the head-foot axis.\",\n            action='store_true',\n            default=False,\n        )\n        parser.add_argument('--augment-noise',\n            help=\"Augment the training data by randomly adding noise to the data.\",\n            action='store_true',\n            default=False,\n        )\n\n        parser.add_argument('--tb-prefix',\n            default='p2ch12',\n            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n        )\n        parser.add_argument('comment',\n            help=\"Comment suffix for Tensorboard run.\",\n            nargs='?',\n            default='dlwpt',\n        )\n\n        self.cli_args = parser.parse_args(sys_argv)\n        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n\n        self.trn_writer = None\n        self.val_writer = None\n        self.totalTrainingSamples_count = 0\n\n        self.augmentation_dict = {}\n        if self.cli_args.augmented or self.cli_args.augment_flip:\n            self.augmentation_dict['flip'] = True\n        if self.cli_args.augmented or self.cli_args.augment_offset:\n            self.augmentation_dict['offset'] = 0.1\n        if self.cli_args.augmented or self.cli_args.augment_scale:\n            self.augmentation_dict['scale'] = 0.2\n        if self.cli_args.augmented or self.cli_args.augment_rotate:\n            self.augmentation_dict['rotate'] = True\n        if self.cli_args.augmented or self.cli_args.augment_noise:\n            self.augmentation_dict['noise'] = 25.0\n\n        self.use_cuda = torch.cuda.is_available()\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n        self.model = self.initModel()\n        self.optimizer = self.initOptimizer()\n\n\n    def initModel(self):\n        model = LunaModel()\n        if self.use_cuda:\n            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n            if torch.cuda.device_count() > 1:\n                model = nn.DataParallel(model)\n            model = model.to(self.device)\n        return model\n\n    def initOptimizer(self):\n        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n        # return Adam(self.model.parameters())\n\n    def initTrainDl(self):\n        train_ds = LunaDataset(\n            val_stride=10,\n            isValSet_bool=False,\n            ratio_int=int(self.cli_args.balanced),\n            augmentation_dict=self.augmentation_dict,\n        )\n\n        batch_size = self.cli_args.batch_size\n        if self.use_cuda:\n            batch_size *= torch.cuda.device_count()\n\n        train_dl = DataLoader(\n            train_ds,\n            batch_size=batch_size,\n            #num_workers=self.cli_args.num_workers,\n            #pin_memory=self.use_cuda,\n        )\n\n        return train_dl\n\n    def initValDl(self):\n        val_ds = LunaDataset(\n            val_stride=10,\n            isValSet_bool=True,\n        )\n\n        batch_size = self.cli_args.batch_size\n        if self.use_cuda:\n            batch_size *= torch.cuda.device_count()\n\n        val_dl = DataLoader(\n            val_ds,\n            batch_size=batch_size,\n            #num_workers=self.cli_args.num_workers,\n            #pin_memory=self.use_cuda,\n        )\n\n        return val_dl\n\n    def initTensorboardWriters(self):\n        if self.trn_writer is None:\n            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n\n            self.trn_writer = SummaryWriter(\n                log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n            self.val_writer = SummaryWriter(\n                log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n\n\n    def main(self):\n        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n\n        train_dl = self.initTrainDl()\n        val_dl = self.initValDl()\n\n        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n\n            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n                epoch_ndx,\n                self.cli_args.epochs,\n                len(train_dl),\n                len(val_dl),\n                self.cli_args.batch_size,\n                (torch.cuda.device_count() if self.use_cuda else 1),\n            ))\n\n            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n\n            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n\n        if hasattr(self, 'trn_writer'):\n            self.trn_writer.close()\n            self.val_writer.close()\n\n\n    def doTraining(self, epoch_ndx, train_dl):\n        self.model.train()\n        train_dl.dataset.shuffleSamples()\n        trnMetrics_g = torch.zeros(\n            METRICS_SIZE,\n            len(train_dl.dataset),\n            device=self.device,\n        )\n\n        batch_iter = enumerateWithEstimate(\n            train_dl,\n            \"E{} Training\".format(epoch_ndx),\n            start_ndx=train_dl.num_workers,\n        )\n        for batch_ndx, batch_tup in tqdm(batch_iter, unit_scale=True):\n            self.optimizer.zero_grad()\n\n            loss_var = self.computeBatchLoss(\n                batch_ndx,\n                batch_tup,\n                train_dl.batch_size,\n                trnMetrics_g,\n            )\n\n            loss_var.backward()\n            self.optimizer.step()\n\n        self.totalTrainingSamples_count += len(train_dl.dataset)\n\n        return trnMetrics_g.to('cpu')\n\n\n    def doValidation(self, epoch_ndx, val_dl):\n        with torch.no_grad():\n            self.model.eval()\n            valMetrics_g = torch.zeros(\n                METRICS_SIZE,\n                len(val_dl.dataset),\n                device=self.device,\n            )\n\n            batch_iter = enumerateWithEstimate(\n                val_dl,\n                \"E{} Validation \".format(epoch_ndx),\n                start_ndx=val_dl.num_workers,\n            )\n            for batch_ndx, batch_tup in batch_iter:\n                self.computeBatchLoss(\n                    batch_ndx,\n                    batch_tup,\n                    val_dl.batch_size,\n                    valMetrics_g,\n                )\n\n        return valMetrics_g.to('cpu')\n\n\n\n    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n        input_t, label_t, _series_list, _center_list = batch_tup\n\n        input_g = input_t.to(self.device, non_blocking=True)\n        label_g = label_t.to(self.device, non_blocking=True)\n\n        logits_g, probability_g = self.model(input_g)\n\n        loss_func = nn.CrossEntropyLoss(reduction='none')\n        loss_g = loss_func(\n            logits_g,\n            label_g[:,1],\n        )\n        start_ndx = batch_ndx * batch_size\n        end_ndx = start_ndx + label_t.size(0)\n\n        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1]\n        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1]\n        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n\n        return loss_g.mean()\n\n\n    def logMetrics(\n            self,\n            epoch_ndx,\n            mode_str,\n            metrics_t,\n            classificationThreshold=0.5,\n    ):\n        self.initTensorboardWriters()\n        log.info(\"E{} {}\".format(\n            epoch_ndx,\n            type(self).__name__,\n        ))\n\n        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n\n        posLabel_mask = ~negLabel_mask\n        posPred_mask = ~negPred_mask\n\n        neg_count = int(negLabel_mask.sum())\n        pos_count = int(posLabel_mask.sum())\n\n        trueNeg_count = neg_correct = int((negLabel_mask & negPred_mask).sum())\n        truePos_count = pos_correct = int((posLabel_mask & posPred_mask).sum())\n\n        falsePos_count = neg_count - neg_correct\n        falseNeg_count = pos_count - pos_correct\n\n        metrics_dict = {}\n        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n\n        metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n        metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n        metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n\n        precision = metrics_dict['pr/precision'] = \\\n            truePos_count / np.float32(truePos_count + falsePos_count)\n        recall    = metrics_dict['pr/recall'] = \\\n            truePos_count / np.float32(truePos_count + falseNeg_count)\n\n        metrics_dict['pr/f1_score'] = \\\n            2 * (precision * recall) / (precision + recall)\n\n        log.info(\n            (\"E{} {:8} {loss/all:.4f} loss, \"\n                 + \"{correct/all:-5.1f}% correct, \"\n                 + \"{pr/precision:.4f} precision, \"\n                 + \"{pr/recall:.4f} recall, \"\n                 + \"{pr/f1_score:.4f} f1 score\"\n            ).format(\n                epoch_ndx,\n                mode_str,\n                **metrics_dict,\n            )\n        )\n        log.info(\n            (\"E{} {:8} {loss/neg:.4f} loss, \"\n                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n            ).format(\n                epoch_ndx,\n                mode_str + '_neg',\n                neg_correct=neg_correct,\n                neg_count=neg_count,\n                **metrics_dict,\n            )\n        )\n        log.info(\n            (\"E{} {:8} {loss/pos:.4f} loss, \"\n                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n            ).format(\n                epoch_ndx,\n                mode_str + '_pos',\n                pos_correct=pos_correct,\n                pos_count=pos_count,\n                **metrics_dict,\n            )\n        )\n        writer = getattr(self, mode_str + '_writer')\n\n        for key, value in metrics_dict.items():\n            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n\n        writer.add_pr_curve(\n            'pr',\n            metrics_t[METRICS_LABEL_NDX],\n            metrics_t[METRICS_PRED_NDX],\n            self.totalTrainingSamples_count,\n        )\n\n        bins = [x/50.0 for x in range(51)]\n\n        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n\n        if negHist_mask.any():\n            writer.add_histogram(\n                'is_neg',\n                metrics_t[METRICS_PRED_NDX, negHist_mask],\n                self.totalTrainingSamples_count,\n                bins=bins,\n            )\n        if posHist_mask.any():\n            writer.add_histogram(\n                'is_pos',\n                metrics_t[METRICS_PRED_NDX, posHist_mask],\n                self.totalTrainingSamples_count,\n                bins=bins,\n            )\n\n        # score = 1 \\\n        #     + metrics_dict['pr/f1_score'] \\\n        #     - metrics_dict['loss/mal'] * 0.01 \\\n        #     - metrics_dict['loss/all'] * 0.0001\n        #\n        # return score\n\n    # def logModelMetrics(self, model):\n    #     writer = getattr(self, 'trn_writer')\n    #\n    #     model = getattr(model, 'module', model)\n    #\n    #     for name, param in model.named_parameters():\n    #         if param.requires_grad:\n    #             min_data = float(param.data.min())\n    #             max_data = float(param.data.max())\n    #             max_extent = max(abs(min_data), abs(max_data))\n    #\n    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n    #\n    #             try:\n    #                 writer.add_histogram(\n    #                     name.rsplit('.', 1)[-1] + '/' + name,\n    #                     param.data.cpu().numpy(),\n    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n    #                     self.totalTrainingSamples_count,\n    #                     # bins=bins,\n    #                 )\n    #             except Exception as e:\n    #                 log.error([min_data, max_data])\n    #                 raise\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:35:57.002599Z","iopub.execute_input":"2024-06-13T09:35:57.002868Z","iopub.status.idle":"2024-06-13T09:36:08.828419Z","shell.execute_reply.started":"2024-06-13T09:35:57.002845Z","shell.execute_reply":"2024-06-13T09:36:08.827599Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-06-13 09:35:59.269099: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 09:35:59.269224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 09:35:59.385528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-06-13 09:36:08,522 INFO     pid:34 numexpr.utils:161:_init_num_threads NumExpr defaulting to 4 threads.\n","output_type":"stream"}]},{"cell_type":"code","source":"class LunaPrepCacheApp:\n    @classmethod\n    def __init__(self, sys_argv=None):\n        if sys_argv is None:\n            sys_argv = sys.argv[1:]\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--batch-size',\n            help='Batch size to use for training',\n            default=1024,\n            type=int,\n        )\n        parser.add_argument('--num-workers',\n            help='Number of worker processes for background data loading',\n            default=8,\n            type=int,\n        )\n\n        self.cli_args = parser.parse_args(sys_argv)\n\n    def main(self):\n        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n\n        self.prep_dl = DataLoader(\n            LunaDataset(\n                sortby_str='series_uid',\n            ),\n            batch_size=self.cli_args.batch_size,\n            num_workers=self.cli_args.num_workers,\n        )\n\n        batch_iter = enumerateWithEstimate(\n            self.prep_dl,\n            \"Stuffing cache\",\n            start_ndx=self.prep_dl.num_workers,\n        )\n        for _ in batch_iter:\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:36:08.829619Z","iopub.execute_input":"2024-06-13T09:36:08.830287Z","iopub.status.idle":"2024-06-13T09:36:08.838751Z","shell.execute_reply.started":"2024-06-13T09:36:08.830254Z","shell.execute_reply":"2024-06-13T09:36:08.837844Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"run(LunaPrepCacheApp)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:36:08.841276Z","iopub.execute_input":"2024-06-13T09:36:08.841535Z","iopub.status.idle":"2024-06-13T09:40:27.231529Z","shell.execute_reply.started":"2024-06-13T09:36:08.841512Z","shell.execute_reply":"2024-06-13T09:40:27.230662Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-06-13 09:36:08,865 INFO     pid:34 __main__:004:run Running: <class '__main__.LunaPrepCacheApp'>(['--num-workers=4']).main()\n2024-06-13 09:36:08,866 INFO     pid:34 __main__:022:main Starting LunaPrepCacheApp, Namespace(batch_size=1024, num_workers=4)\n2024-06-13 09:36:11,765 INFO     pid:34 __main__:258:__init__ <__main__.LunaDataset object at 0x7871aac76e30>: 56938 training samples, 56816 neg, 122 pos, unbalanced ratio\n2024-06-13 09:36:11,767 WARNING  pid:34 __main__:220:enumerateWithEstimate Stuffing cache ----/56, starting\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n2024-06-13 09:36:55,337 INFO     pid:34 __main__:237:enumerateWithEstimate Stuffing cache    8/56, done at 2024-06-13 09:40:55, 0:04:25\n2024-06-13 09:37:39,418 INFO     pid:34 __main__:237:enumerateWithEstimate Stuffing cache   16/56, done at 2024-06-13 09:41:08, 0:04:38\n2024-06-13 09:38:44,138 INFO     pid:34 __main__:237:enumerateWithEstimate Stuffing cache   32/56, done at 2024-06-13 09:40:30, 0:04:00\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n2024-06-13 09:40:27,202 WARNING  pid:34 __main__:250:enumerateWithEstimate Stuffing cache ----/56, done at 2024-06-13 09:40:27\n2024-06-13 09:40:27,227 INFO     pid:34 __main__:009:run Finished: <class '__main__.LunaPrepCacheApp'>.['--num-workers=4']).main()\n","output_type":"stream"}]},{"cell_type":"code","source":"run(LunaTrainingApp, \"--epochs=5\", \"--batch-size=32\", \"--balanced\", \"--augmented\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:40:27.233492Z","iopub.execute_input":"2024-06-13T09:40:27.233888Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-06-13 09:40:27,235 INFO     pid:34 __main__:004:run Running: <class '__main__.LunaTrainingApp'>(['--num-workers=4', '--epochs=5', '--batch-size=32', '--balanced', '--augmented']).main()\n2024-06-13 09:40:27,371 INFO     pid:34 __main__:123:initModel Using CUDA; 1 devices.\n2024-06-13 09:40:28,698 INFO     pid:34 __main__:184:main Starting LunaTrainingApp, Namespace(batch_size=32, num_workers=4, epochs=5, balanced=True, augmented=True, augment_flip=False, augment_offset=False, augment_scale=False, augment_rotate=False, augment_noise=False, tb_prefix='p2ch12', comment='dlwpt')\n2024-06-13 09:40:28,753 INFO     pid:34 __main__:258:__init__ <__main__.LunaDataset object at 0x787164c77010>: 51244 training samples, 51135 neg, 109 pos, 1:1 ratio\n2024-06-13 09:40:28,761 INFO     pid:34 __main__:258:__init__ <__main__.LunaDataset object at 0x787122904490>: 5694 validation samples, 5681 neg, 13 pos, unbalanced ratio\n2024-06-13 09:40:28,761 INFO     pid:34 __main__:191:main Epoch 1 of 5, 1563/178 batches of size 32*1\n0.00it [00:00, ?it/s]2024-06-13 09:40:28,830 WARNING  pid:34 __main__:220:enumerateWithEstimate E1 Training ----/1563, starting\n5.00it [00:08, 1.14s/it]2024-06-13 09:40:37,541 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Training    4/1563, done at 2024-06-13 10:25:51, 0:45:22\n17.0it [00:11, 4.17it/s]2024-06-13 09:40:40,637 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Training   16/1563, done at 2024-06-13 09:58:34, 0:18:05\n65.0it [00:24, 4.39it/s]2024-06-13 09:40:53,584 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Training   64/1563, done at 2024-06-13 09:50:24, 0:09:55\n257it [01:18, 4.22it/s] 2024-06-13 09:41:46,897 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Training  256/1563, done at 2024-06-13 09:48:23, 0:07:54\n1.02kit [04:50, 3.47it/s]2024-06-13 09:45:19,740 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Training 1024/1563, done at 2024-06-13 09:47:52, 0:07:23\n1.56kit [07:25, 4.82it/s]2024-06-13 09:47:53,840 WARNING  pid:34 __main__:250:enumerateWithEstimate E1 Training ----/1563, done at 2024-06-13 09:47:53\n1.56kit [07:25, 3.51it/s]\n2024-06-13 09:47:53,884 INFO     pid:34 __main__:300:logMetrics E1 LunaTrainingApp\n2024-06-13 09:47:53,899 INFO     pid:34 __main__:337:logMetrics E1 trn      0.4576 loss,  77.2% correct, 0.8012 precision, 0.7226 recall, 0.7599 f1 score\n2024-06-13 09:47:53,899 INFO     pid:34 __main__:349:logMetrics E1 trn_neg  0.4172 loss,  82.1% correct (20516 of 25000)\n2024-06-13 09:47:53,901 INFO     pid:34 __main__:360:logMetrics E1 trn_pos  0.4981 loss,  72.3% correct (18066 of 25000)\n2024-06-13 09:47:53,953 WARNING  pid:34 __main__:220:enumerateWithEstimate E1 Validation  ----/178, starting\n2024-06-13 09:47:54,968 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Validation     4/178, done at 2024-06-13 09:48:30, 0:00:36\n2024-06-13 09:47:57,859 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Validation    16/178, done at 2024-06-13 09:48:34, 0:00:40\n2024-06-13 09:48:07,225 INFO     pid:34 __main__:237:enumerateWithEstimate E1 Validation    64/178, done at 2024-06-13 09:48:30, 0:00:36\n2024-06-13 09:48:34,420 WARNING  pid:34 __main__:250:enumerateWithEstimate E1 Validation  ----/178, done at 2024-06-13 09:48:34\n2024-06-13 09:48:34,427 INFO     pid:34 __main__:300:logMetrics E1 LunaTrainingApp\n2024-06-13 09:48:34,429 INFO     pid:34 __main__:337:logMetrics E1 val      0.3663 loss,  90.0% correct, 0.0207 precision, 0.9231 recall, 0.0405 f1 score\n2024-06-13 09:48:34,432 INFO     pid:34 __main__:349:logMetrics E1 val_neg  0.3668 loss,  90.0% correct (5113 of 5681)\n2024-06-13 09:48:34,433 INFO     pid:34 __main__:360:logMetrics E1 val_pos  0.1568 loss,  92.3% correct (12 of 13)\n2024-06-13 09:48:34,449 INFO     pid:34 __main__:191:main Epoch 2 of 5, 1563/178 batches of size 32*1\n0.00it [00:00, ?it/s]2024-06-13 09:48:34,544 WARNING  pid:34 __main__:220:enumerateWithEstimate E2 Training ----/1563, starting\n5.00it [00:01, 4.61it/s]2024-06-13 09:48:35,623 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Training    4/1563, done at 2024-06-13 09:54:11, 0:05:37\n17.0it [00:05, 3.67it/s]2024-06-13 09:48:39,815 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Training   16/1563, done at 2024-06-13 09:56:39, 0:08:04\n65.0it [00:18, 4.19it/s]2024-06-13 09:48:53,074 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Training   64/1563, done at 2024-06-13 09:56:00, 0:07:25\n257it [01:12, 3.34it/s] 2024-06-13 09:49:46,926 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Training  256/1563, done at 2024-06-13 09:55:54, 0:07:20\n1.02kit [04:56, 2.74it/s]2024-06-13 09:53:30,701 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Training 1024/1563, done at 2024-06-13 09:56:06, 0:07:31\n1.56kit [07:50, 4.58it/s]2024-06-13 09:56:25,065 WARNING  pid:34 __main__:250:enumerateWithEstimate E2 Training ----/1563, done at 2024-06-13 09:56:25\n1.56kit [07:50, 3.32it/s]\n2024-06-13 09:56:25,162 INFO     pid:34 __main__:300:logMetrics E2 LunaTrainingApp\n2024-06-13 09:56:25,166 INFO     pid:34 __main__:337:logMetrics E2 trn      0.3515 loss,  85.4% correct, 0.9091 precision, 0.7875 recall, 0.8439 f1 score\n2024-06-13 09:56:25,167 INFO     pid:34 __main__:349:logMetrics E2 trn_neg  0.3041 loss,  92.1% correct (23031 of 25000)\n2024-06-13 09:56:25,168 INFO     pid:34 __main__:360:logMetrics E2 trn_pos  0.3988 loss,  78.7% correct (19687 of 25000)\n2024-06-13 09:56:25,181 WARNING  pid:34 __main__:220:enumerateWithEstimate E2 Validation  ----/178, starting\n2024-06-13 09:56:25,956 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Validation     4/178, done at 2024-06-13 09:56:52, 0:00:27\n2024-06-13 09:56:27,271 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Validation    16/178, done at 2024-06-13 09:56:47, 0:00:21\n2024-06-13 09:56:41,280 INFO     pid:34 __main__:237:enumerateWithEstimate E2 Validation    64/178, done at 2024-06-13 09:57:09, 0:00:44\n2024-06-13 09:56:58,439 WARNING  pid:34 __main__:250:enumerateWithEstimate E2 Validation  ----/178, done at 2024-06-13 09:56:58\n2024-06-13 09:56:58,453 INFO     pid:34 __main__:300:logMetrics E2 LunaTrainingApp\n2024-06-13 09:56:58,454 INFO     pid:34 __main__:337:logMetrics E2 val      0.2915 loss,  93.8% correct, 0.0276 precision, 0.7692 recall, 0.0533 f1 score\n2024-06-13 09:56:58,455 INFO     pid:34 __main__:349:logMetrics E2 val_neg  0.2912 loss,  93.8% correct (5329 of 5681)\n2024-06-13 09:56:58,456 INFO     pid:34 __main__:360:logMetrics E2 val_pos  0.4360 loss,  76.9% correct (10 of 13)\n2024-06-13 09:56:58,463 INFO     pid:34 __main__:191:main Epoch 3 of 5, 1563/178 batches of size 32*1\n0.00it [00:00, ?it/s]2024-06-13 09:56:58,511 WARNING  pid:34 __main__:220:enumerateWithEstimate E3 Training ----/1563, starting\n5.00it [00:01, 4.61it/s]2024-06-13 09:56:59,596 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Training    4/1563, done at 2024-06-13 10:02:37, 0:05:38\n17.0it [00:04, 3.36it/s]2024-06-13 09:57:02,710 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Training   16/1563, done at 2024-06-13 10:03:24, 0:06:25\n65.0it [00:17, 3.76it/s]2024-06-13 09:57:16,278 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Training   64/1563, done at 2024-06-13 10:04:05, 0:07:07\n257it [01:12, 2.88it/s] 2024-06-13 09:58:11,380 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Training  256/1563, done at 2024-06-13 10:04:21, 0:07:23\n1.02kit [05:03, 2.51it/s]2024-06-13 10:02:01,711 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Training 1024/1563, done at 2024-06-13 10:04:40, 0:07:42\n1.56kit [07:57, 3.50it/s]2024-06-13 10:04:56,329 WARNING  pid:34 __main__:250:enumerateWithEstimate E3 Training ----/1563, done at 2024-06-13 10:04:56\n1.56kit [07:57, 3.27it/s]\n2024-06-13 10:04:56,420 INFO     pid:34 __main__:300:logMetrics E3 LunaTrainingApp\n2024-06-13 10:04:56,423 INFO     pid:34 __main__:337:logMetrics E3 trn      0.3271 loss,  86.8% correct, 0.9248 precision, 0.8002 recall, 0.8580 f1 score\n2024-06-13 10:04:56,424 INFO     pid:34 __main__:349:logMetrics E3 trn_neg  0.2747 loss,  93.5% correct (23374 of 25000)\n2024-06-13 10:04:56,424 INFO     pid:34 __main__:360:logMetrics E3 trn_pos  0.3796 loss,  80.0% correct (20004 of 25000)\n2024-06-13 10:04:56,447 WARNING  pid:34 __main__:220:enumerateWithEstimate E3 Validation  ----/178, starting\n2024-06-13 10:04:56,880 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Validation     4/178, done at 2024-06-13 10:05:11, 0:00:15\n2024-06-13 10:04:58,364 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Validation    16/178, done at 2024-06-13 10:05:16, 0:00:20\n2024-06-13 10:05:02,976 INFO     pid:34 __main__:237:enumerateWithEstimate E3 Validation    64/178, done at 2024-06-13 10:05:14, 0:00:17\n2024-06-13 10:05:15,618 WARNING  pid:34 __main__:250:enumerateWithEstimate E3 Validation  ----/178, done at 2024-06-13 10:05:15\n2024-06-13 10:05:15,631 INFO     pid:34 __main__:300:logMetrics E3 LunaTrainingApp\n2024-06-13 10:05:15,633 INFO     pid:34 __main__:337:logMetrics E3 val      0.3528 loss,  91.8% correct, 0.0251 precision, 0.9231 recall, 0.0488 f1 score\n2024-06-13 10:05:15,633 INFO     pid:34 __main__:349:logMetrics E3 val_neg  0.3532 loss,  91.8% correct (5214 of 5681)\n2024-06-13 10:05:15,634 INFO     pid:34 __main__:360:logMetrics E3 val_pos  0.1788 loss,  92.3% correct (12 of 13)\n2024-06-13 10:05:15,642 INFO     pid:34 __main__:191:main Epoch 4 of 5, 1563/178 batches of size 32*1\n0.00it [00:00, ?it/s]2024-06-13 10:05:15,690 WARNING  pid:34 __main__:220:enumerateWithEstimate E4 Training ----/1563, starting\n5.00it [00:01, 2.89it/s]2024-06-13 10:05:17,265 INFO     pid:34 __main__:237:enumerateWithEstimate E4 Training    4/1563, done at 2024-06-13 10:13:27, 0:08:12\n17.0it [00:05, 2.69it/s]2024-06-13 10:05:21,299 INFO     pid:34 __main__:237:enumerateWithEstimate E4 Training   16/1563, done at 2024-06-13 10:13:51, 0:08:35\n65.0it [00:20, 3.72it/s]2024-06-13 10:05:36,309 INFO     pid:34 __main__:237:enumerateWithEstimate E4 Training   64/1563, done at 2024-06-13 10:13:31, 0:08:15\n257it [01:15, 4.62it/s] 2024-06-13 10:06:31,689 INFO     pid:34 __main__:237:enumerateWithEstimate E4 Training  256/1563, done at 2024-06-13 10:12:57, 0:07:42\n582it [02:55, 1.42it/s]","output_type":"stream"}]}]}